\section{Conclusions}
\label{conclusions}
Two paragraphs of best paper awards goes here.

% Introducing globally visible shared memory in future CPU--GPU systems
% improves programmer productivity and significantly reduces the barrier
% to entry of using such systems for many applications. 
% Hardware cache coherence can provide such shared memory and
% extend the benefits of on-chip caching to all system memory.
% \ignore{Hardware cache coherence in future CPU/GPU systems would allow the integration
% of the separate CPU and GPU programming paradigms under a single
% uniform model, leveraging the benefits of on-chip caching for all
% memory within the system.}  However, extending hardware cache coherence 
% throughout the GPU places enormous
% scalability demands on the coherence implementation.  Moreover, integrating
% discrete processors, possibly designed by distinct vendors,
% into a single coherence protocol is a prohibitive engineering and
% verification challenge.  
% 
% We demonstrate that CPU--GPU hardware cache coherence is not needed to achieve the
% simultaneous goals of unified shared memory and high GPU performance.  
% We show that \textit{selective caching} with request coalescing,
% a CPU-side GPU client cache and variable-sized transfer units
% can perform within 93\% of a
% cache-coherent GPU for applications that do not perform fine
% grained CPU--GPU data sharing and synchronization. We also show that promiscuous
% read-only caching benefits memory latency-sensitive applications by using
% OS page-protection mechanisms rather than relying on hardware cache coherence.  Selective caching
% does not needlessly force hardware cache coherence into the GPU memory system,
% allowing decoupled designs that can maximize CPU and GPU performance, while
% still maintaining the CPU's traditional view of\ignore{ a hardware coherent} the
% memory system.



\begin{table}[t]
\begin{small}
\centering
\begin{tabular}{lccc}
 \toprule
 \textbf{Benchmark} & \textbf{Kernels} & \textbf{Time-weighted} & \textbf{Memory} \\
& & \textbf{Average CTAs} & \textbf{(MB)} \\
 \toprule
ML-GoogLeNet-cudnn-Lev2 & 1 & 6272 & 1205 \\
ML-AlexNet-cudnn-Lev2 & 1 & 1250 & 832 \\
ML-OverFeat-cudann-Lev3 & 1 & 1800 & 388 \\
ML-AlexNet-cudnn-Lev4 & 1 & 1014 & 32 \\
ML-AlexNet-ConvNet2 & 1 & 6075 & 97 \\
Rodinia-Backprop & 2 & 4096 & 160 \\
Rodinia-Euler3D & 346 & 1008 & 25 \\
Rodinia-BFS & 24 & 1954 & 38 \\
Rodinia-Gaussian & 510 & 2599 & 78 \\
Rodinia-Hotspot & 1 & 7396 & 64 \\
Rodinia-Kmeans & 3 & 3249 & 221 \\
Rodnia-Pathfinder & 20 & 4630 & 1570 \\
Rodinia-Srad & 4 & 16384 & 98 \\
HPC-SNAP & 118 & 200 & 744 \\
HPC-Nekbone-Large & 300 & 5583 & 294 \\
HPC-MiniAMR & 33 & 76033 & 2752 \\
HPC-MiniContact-Mesh1 & 500 & 250 & 21 \\
HPC-MiniContact-Mesh2 & 127 & 15423 & 257 \\
HPC-Lulesh-Unstruct-Mesh1 & 2000 & 435 & 19 \\
HPC-Lulesh-Unstruct-Mesh2 & 200 & 4940 & 208 \\
HPC-AMG & 88 & 241549 & 3744 \\
HPC-RSBench & 1 & 7813 & 19 \\
HPC-MCB & 1 & 5001 & 162 \\
HPC-NAMD2.9 & 1 & 3888 & 88 \\
HPC-RabbitCT & 1 & 131072 & 524 \\
HPC-Lulesh & 105 & 12202 & 578 \\
HPC-CoMD & 350 & 3588 & 319 \\
HPC-CoMD-Wa & 350 & 13691 & 393 \\
HPC-CoMD-Ta & 350 & 5724 & 394 \\
HPC-HPGMG-UVM & 359 & 10436 & 1975 \\
HPC-HPGMG & 317 & 10506 & 1571 \\
%HPC-HPGMG-UVM-Base & 359 & 10728 & 1975 \\
Lonestar-SP & 11 & 75 & 8 \\
Lonestar-MST-Graph & 87 & 770 & 86 \\
Lonestar-MST-Mesh & 71 & 895 & 75 \\
%HPC-Nekbone-medium & 510 & 3093 & 170 \\
Lonestar-SSSP-Wln & 1000 & 60 & 21 \\
Lonestar-DMR & 3 & 82 & 248 \\
Lonestar-SSSP-Wlc & 1300 & 163 & 21 \\
Lonestar-SSSP & 102 & 1046 & 38 \\
Other-Stream-Triad & 5 & 699051 & 3146 \\
Other-Optix-Raytracing & 1 & 3072 & 87 \\
Other-Bitcoin-Crypto & 1 & 60 & 5898 \\
\toprule
\end{tabular}
\caption{Application footprint and average number of CTAs (thread blocks) 
available during time-weighted execution.}
\label{tab:numctas}
\end{small}
\end{table}