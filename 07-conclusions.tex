\section{Conclusions}
\label{conclusions}
With transistors growth slowing and multi-GPU programming requiring 
re-architecting of GPU applications, the future of scalable single GPU 
performance is in question.  We propose that much like CPU designs have done in 
the past, the natural progression for continuous performance scalability of 
traditional GPU workloads is to move from a single to multi-socket NUMA design.  
In this work we show 
that applying NUMA scheduling and memory placement policies inherited 
from the CPU world is not sufficient to achieve good performance scalability.  
We show that future GPU designs will need to become NUMA-aware both in their 
interconnect management and within their caching subsystems to overcome the 
inherent performance penalty that NUMA memory systems introduce.  By leveraging 
software policies that preserve data locality and hardware policies that can 
dynamically adapt to application phases, our NUMA-aware multi-socket GPU is able to 
outperform current single GPUs designs by XXX\%, XXX\%, and XXX\% and 
achieves XXX\%, XXX\%, and XXX\% of theoretical application scalability in 2, 4, 
and 8 socket designs respectively.  This scalable performance indicates that the 
challenges of designing a multi-socket NUMA GPU can be overcome through a combination
of runtime and architectural optimization, making 
NUMA-aware GPUs a promising technology for scaling GPU performance beyond a 
single socket.


