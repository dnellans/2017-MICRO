\vspace{-0.1in}
\section{Conclusions}
\label{conclusions}
With transistors growth slowing and multi-GPU programming requiring 
re-architecting of GPU applications, the future of scalable single GPU 
performance is in question.  We propose that much like CPU designs have done in 
the past, the natural progression for continuous performance scalability of 
traditional GPU workloads is to move from a single to multi-socket NUMA design.  
In this work we show 
that applying NUMA scheduling and memory placement policies inherited 
from the CPU world is not sufficient to achieve good performance scalability.  
We show that future GPU designs will need to become NUMA-aware both in their 
interconnect management and within their caching subsystems to overcome the 
inherent performance penalty that NUMA memory systems introduce.  By leveraging 
software policies that preserve data locality and hardware policies that can 
dynamically adapt to application phases, our proposed NUMA-aware multi-socket GPU is able to 
outperform current GPU designs by 1.5$\times$, 2.3$\times$, and
3.2$\times$, while achieving 89\%, 84\%, and 76\% of theoretical application scalability in 2, 4, 
and 8 GPU sockets respectively. Our results indicate that the 
challenges of designing a multi-socket NUMA GPU can be solved through a combination
of runtime and architectural optimization, making 
NUMA-aware GPUs a promising technology for scaling GPU performance beyond a 
single socket.


