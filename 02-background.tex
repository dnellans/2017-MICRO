%newpage here since motivation usualy starts on page3
\newpage
\section{Motivation and Background}
\label{background}

Motivate moore's law slowing, we want continuous performance scalability, etc.

About transparency. Multi-GPU systems are here but besides HPC (maybe graphics 
as well) which knows how to use them efficiently (MPI process per GPU), the rest 
of the market needs to re-think, re-write, and re-compile its code. We advocate 
for the runtime which allows a kernel to be partitioned into sub-kernels and 
distributes each of them to a different GPU device. In the future, the number of 
GPUs and inter-GPU network complexity is going to increase, so we need to hide 
that from a programmer.
Problems of NUMA systems are well known in CPU world. Remote traffic is a 
problem which can be solved either with some dynamic memory page allocation 
policy (migrations and replications) or NUMA-aware thread scheduling. We focus 
on the architectural tweaks.


Chart supports the fact that we have plenty of CTAs to fill a multi-GPU without a problem
and that our memory footprints are big enough to not be cache resident in this study.

\begin{table}[t]
\begin{center}
\begin{tabular}{ddd}
 \hline
 \multicolumn{1}{l}{Workload} &   \multicolumn{1}{c}{Number of CTAs } &  \multicolumn{1}{c}{Mem. Footprint (MB)}\\
 \hline
 \hline
 \multicolumn{1}{l}{backprop}  &   62 &   62.4 \\
 \hline
 \multicolumn{1}{l}{bfs}  &   19&   62.4  \\
 \hline
 \multicolumn{1}{l}{btree}  &   81 &   62.4 \\
 \hline
 \multicolumn{1}{l}{cns}  &   47  &   62.4  \\
 \hline
 \multicolumn{1}{l}{comd}  &   62 &   62.4 \\
 \hline
 \multicolumn{1}{l}{kmeans}  &   5 &   62.4 \\
 \hline
 \multicolumn{1}{l}{minife}  &   46 &   62.4 \\
 \hline
 \multicolumn{1}{l}{mummer}  &   60 &   62.4 \\
 \hline
 \multicolumn{1}{l}{needle}  &   7  &   62.4  \\
 \hline
 \multicolumn{1}{l}{pathfinder}  &   42 &   62.4 \\
 \hline
 \multicolumn{1}{l}{srad\_v1}  &   46 &   62.4 \\
 \hline
 \multicolumn{1}{l}{xsbench}  &   30 &   62.4 \\
\hline
\end{tabular}
\caption{Number of CTAs and total memory footprint when compared to multi-socket GPU with 256 active CTA contexts containing XXX MB of LLC.}
\label{tab:numctas}
\end{center}
\end{table}



\subsection{A Multi-Socket GPU Runtime}

Oreste going to write section here with all the details of what needs to be done
to make this all work in our basic multi-socket GPU runtime.  Remember we're not
claiming this as contribution, just background.

Figure in this section explains why we can't just do fine grained memory interleaving.
nvlink bandwidth kills us.  additional explain on the fact that CTA scheduling at round robin
is impossible so we do some minimal batching but then we lose locality, so we statically
partition 1/4 each and use first touch

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/motivation1.jpg}
    \caption{Multi-socket GPU performance by applying known CPU NUMA localization techniques for scheduling and memory placement.}
    \label{fig:motivation}
\end{figure}

Second figure now shows our baseline we're going to compare against - for some benchmarks we can't expect speedup, its quite good already.
For others there is a long ways to go and we're going to try and close that gap now.

\begin{figure}[tp]
    \centering
    \includegraphics[width=1\linewidth]{figures/plot_scalability_baseline.pdf}
    \caption{Relative performance of multi-socket GPU to an unbuildable 4x larger GPU.}
    \label{fig:motivation2}
\end{figure}

\subsection{Related Work}

This is going to come up front in this paper because there is a a bunch and we want to explain how
why this paper is different.

Multi-GPU work
Cache partitioning work
We do not optimize for latency (the highest hit rate) but for bandwidth (the hit rate is not the highest; we can show some graph later to support that the best cache configuration does not provide the highest hit rate)
Not sure about NVLink/interconnection related work
Discuss simplex vs duplex interconnects / point-to-point links
At least mention which interconnects / point to point links can have reversible links vs statically partitioned links (Ethernet, Infiniband, QPI)
Is there a reference that says that GRS signal links can reverse direction?


% Heterogeneous CPU--GPU systems have been widely
% adopted by the high performance computing community 
% and are becoming increasingly common in other computing paradigms.  High performance GPUs have 
% developed into stand-alone PCIe-attached accelerators requiring explicit memory 
% management by the programmer to control data transfers into the GPU's 
% high-bandwidth locally attached memory. As GPUs have evolved, the onus of 
% explicit memory management has been addressed by providing a unified shared
% memory address space between the GPU and CPU~\cite{UVM,HSA}.  Whereas a single 
% unified virtual address space improves programmer productivity, discrete GPU and 
% CPU systems still have separate locally attached physical memories, optimized for 
% bandwidth and latency respectively. 
% 
% Managing the physical location of data, and guaranteeing that reads access 
% the most up-to-date copies of 
% data in a unified shared memory can be done through the use of page level 
% migration and protection. Such mechanisms move data at the OS page granularity between 
% physical memories~\cite{UVM}.  With the advent of non-PCIe high-bandwidth, low-latency
% CPU--GPU interconnects, the possibility of performing cache-line, rather than OS-page-granularity, accesses
% becomes feasible.  Without OS page protection
% mechanisms to support correctness guarantees, however,  the responsibility of coherence
% has typically fallen on hardware cache-coherence implementations.
% 
% \ignore{Managing the physical location and coherence guarantee of 
% data in a unified shared memory can be done through the use of page level 
% migration and protection, which moves data at the OS page granularity between 
% physical memories~\cite{UVM}.  With the advent of non-PCIe high bandwidth, low latency,
% CPU--GPU interconnects the possibility of performing cache-line based accesses,
% rather than OS page granularity, becomes feasible.  Without OS page protection
% mechanisms to support shared memory guarantees, however,  the responsibility of coherence
% has typically fallen on hardware cache-coherence implementations.}
% 
% \begin{table}[t]
% \begin{center}
% \begin{tabular}{ddd}
%  \hline
%  \multicolumn{1}{l}{Workload} &   \multicolumn{1}{c}{L1 Hit Rate (\%)}  &  \multicolumn{1}{c}{L2 Hit Rate (\%)}  \\
%  \hline
%  \hline
%  \multicolumn{1}{l}{backprop}  &   62.4  &   70.0\\
%  \hline
%  \multicolumn{1}{l}{bfs}  &   19.6  &   58.6  \\
%  \hline
%  \multicolumn{1}{l}{btree}  &   81.8  &   61.8  \\
%  \hline
%  \multicolumn{1}{l}{cns}  &   47.0  &   55.2  \\
%  \hline
%  \multicolumn{1}{l}{comd}  &   62.5  &   97.1  \\
%  \hline
%  \multicolumn{1}{l}{kmeans}  &   5.6  &   29.5  \\
%  \hline
%  \multicolumn{1}{l}{minife}  &   46.7  &   20.4  \\
%  \hline
%  \multicolumn{1}{l}{mummer}  &   60.0  &   30.0  \\
%  \hline
%  \multicolumn{1}{l}{needle}  &   7.0  &   55.7  \\
%  \hline
%  \multicolumn{1}{l}{pathfinder}  &   42.4  &   23.0  \\
%  \hline
%  \multicolumn{1}{l}{srad\_v1}  &   46.9  &   25.9  \\
%  \hline
%  \multicolumn{1}{l}{xsbench}  &   30.7  &   63.0  \\
%  \hline
%  \hline
%  \multicolumn{1}{l}{Arith Mean}  &   44.4  &   51.6  \\
% \hline
% \end{tabular}
% \caption{GPU L1 and L2 cache hit rates (average).}
% \label{tab:gpuhitrate}
% \end{center}
% \vspace{-.2in}
% \end{table}
% 
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/cache_bw_latency.png}
%     \caption{GPU performance sensitivity to L1 and L2 latency and bandwidth
% changes.}
%     \label{fig:cache_bw_latency}
%     \vspace{-.05in}
% \end{figure*}
% 
% As programming models supporting transparent CPU--GPU sharing become 
% more prevalent and sharing becomes more fine-grain and frequent, the 
% performance gap between page-level coherence and fine-grained hardware cache-coherent
% access will grow~\cite{Agarwal2015,Agarwal2015b,Lim2012}. 
% On-chip caches, and thus HW cache coherence, are widely used in CPUs because they 
% provide substantial memory bandwidth and latency 
% improvements~\cite{Martin2012}.
% Building scalable, high-performance cache coherence requires 
% a holistic system that strikes a balance between directory storage 
% overhead, cache probe bandwidth, and application 
% characteristics~\cite{Power2013,Pugsley2010,Cantin2005,johnson2011,Hong2012,Sanchez2012,Kelm2010}.
% Although relaxed or scoped consistency models allow coherence operations
% to be re-ordered or deferred, hiding latency, they do not obviate the need 
% for HW cache coherence. However, supporting a CPU-like HW coherence model
% in large GPUs, where many applications do not require coherence, is a tax on GPU designers.  Similarly,
% requiring CPUs to relax or change their HW coherence implementations or implement instructions
% enabling software management of the cache hierarchy adds significant system complexity.
% 
% Prior work has shown that due to their many threaded design, GPUs are 
% insensitive to off-package memory latency but very sensitive to off-chip memory 
% bandwidth~\cite{Agarwal2015,Agarwal2015b}. Table~\ref{tab:gpuhitrate}
% shows the L1 and L2 cache hit rates across a variety of workloads from the Rodinia 
% and United States Department of Energy application suites~\cite{Che2009,villa2014}.  These low hit 
% rates cause GPUs to also be fairly
% insensitive to small changes in L1 and L2 cache latency and bandwidth, as shown in 
% Figure~\ref{fig:cache_bw_latency}.  This lack of sensitivity raises the question whether GPUs need 
% to uniformly employ on-chip caching of all off-chip memory to achieve good performance.  If GPUs do not 
% need or can selectively employ on-chip caching, then CPU--GPU systems can be built that
% present a unified, coherent shared-memory address space to the CPU, while not requiring a 
% HW cache-coherence implementation within the GPU. 
% 
% Avoiding hardware cache coherence benefits GPUs by decoupling them from the coherence protocol 
% implemented within the CPU complex, enables simplified GPU designs, and improves
% compatibility across future systems. It also reduces the scaling load on the 
% existing CPU coherence and directory structures by eliminating the potential addition
% of hundreds of caches, all of which may be sharing data. However, selective caching does not come without
% a cost. Some portions of the global memory space will become un-cacheable
% within the GPU\@ and bypassing on-chip caches can place additional load 
% on limited off-chip memory resources.  In the following sections, we show that by leveraging
% memory request coalescing, small CPU-side caches, improved interconnect efficiency, and
% promiscuous read-only caching, selective caching GPUs can perform nearly as well
% as HW cache-coherent CPU--GPU\@ systems.
