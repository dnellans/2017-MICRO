\vspace{-0.1in}
\section{Motivation and Background}
\label{sec:background}

Over the last decade single GPU performance has scaled thanks to a 
significant growth in per-GPU transistor count and DRAM bandwidth. For 
example, in 2010 NVIDIA's Fermi GPU integrated 1.95B transistors on a 
\SI{529}{mm\textsuperscript{2}} die, with \SI{180}{GB/s} of DRAM 
bandwidth~\cite{hennessypatterson11}. In 2016 NVIDIA's Pascal GPU 
contained 12B transistors within a \SI{610}{mm\textsuperscript{2}} die, while 
relying on \SI{720}{GB/s} of memory bandwidth~\cite{inside-pascal}. Unfortunately, 
transistor density is slowing significantly and many integrated circuit 
manufacturers are not providing roadmaps beyond \SI{7}{nm}. Moreover, GPU die sizes,
which have been also slowly but steadily growing generationally, are 
expected to slow down due to limitations in lithography and manufacturing cost.

\begin{figure}[t] 
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/plot_ctas_per_sm.pdf}
    \caption{Percentage of workloads that are able to fill future larger GPUs 
    (average number of concurrent thread blocks exceeds number of SMs in the system).}
    \label{fig:ctas}
    \vspace{-.2in}
\end{figure}

Without either larger or denser dies, GPU architects must turn to 
alternative solutions to significantly increase GPU performance.  Recently 3D 
die-stacking has seen significant interest due to its successes with high 
bandwidth DRAM~\cite{HBM}. Unfortunately 3D die-stacking still has 
a significant engineering challenges related to power delivery, 
energy density, and cooling~\cite{verbree2010cost} when employed in power 
hungry, maximal die-sized chips such as GPUs. Thus we propose GPU manufacturers 
are likely to re-examine a tried and trued solution from CPU world, 
\textit{multi-socket GPUs}, to scaling GPU performance while maintaining the 
current ratio of floating point operations per second (FLOPS) and DRAM 
bandwidth.

Multi-socket GPUs are enabled by the evolution of GPUs from external
peripherals to central computing components, considered at system design time.  
GPU optimized systems now employ custom PCB designs that accommodate 
high pin count socketed GPU modules~\cite{dgx} with inter-GPU interconnects resembling 
QPI or HyperTransport~\cite{NVLINK,INTELQPI,AMDHT}. Despite the rapid 
improvement in hardware capabilities, these systems have 
continued to expose the GPUs provided as individually addressable units. These
multi-GPU systems can provide high aggregate throughput when running multiple concurrent
GPU kernels, but to accelerate a single GPU workload they
require layering additional software runtimes on top of native GPU programming 
interfaces such as CUDA or OpenCL~\cite{CUDA,OPENCL}. Unfortunately, by requiring
application re-design many workloads are never ported to take advantage
of multiple GPUs.

Extending single GPU workload performance significantly is a laudable
goal, but we must first understand if these applications will be able to leverage
larger GPUs. Assuming that the biggest GPU in the market today amasses $\approx$50 SMs (i.e. NVIDIA's Pascal GPU contains
56), Figure~\ref{fig:ctas} shows that across a benchmark set of 41 applications that are later described in Section~\ref{sec:methodology},
 most single GPU optimized workloads already
contain sufficient data parallelism to fill GPUs that are 2--8$\times$ larger 
than today's biggest GPUs. For those that do not, we find that the absolute number
of thread blocks (CTAs) is intentionally limited by the programmer, or that the problem dataset can not
be easily scaled up due to memory limitations.
While most of these applications that do scale are unlikely to scale to thousands of GPUs across an 
entire data center without additional developer effort, moderate programmer transparent performance 
scalability will be
attractive for applications that already contain sufficient algorithmic and data parallelism.

In this work, we examine the performance of a future 4-module NUMA GPU to 
understand the effects that NUMA will have when executing applications designed 
for UMA GPUs. We will show (not surprisingly), that when executing UMA-optimized 
GPU programs on a multi-socket NUMA GPU, performance does not scale.  We draw on 
prior work and show that before optimizing GPU microarchitecture for 
NUMA-awareness, several software optimizations must be in place to preserve data 
locality.  Alone these SW improvements are not sufficient to achieve scalable 
performance however and interconnect bandwidth is a significant hindrance on 
performance.  To overcome this bottleneck we propose two classes of improvements 
to reduce the observed NUMA penalty.

First, in Section~\ref{sec:interconnect} we examine the 
ability of switch connected GPUs to dynamically repartition their ingress and 
egress links to provide asymmetric bandwidth provisioning when required.  By 
using existing interconnects more efficiently, the effective NUMA bandwidth ratio 
of remote memory to local memory decreases, improving performance. Second, 
to minimize traffic on oversubscribed interconnect links we propose GPU caches need 
to become NUMA-aware in Section~\ref{sec:caches}. Traditional on-chip caches are 
optimized to maximize 
overall hit rate, thus minimizing off-chip bandwidth. However, in NUMA 
systems, not all cache misses have the same relative cost and thus should not 
be treated equally. Due to the NUMA penalty of accessing remote memory, we show 
that performance can be maximized by preferencing cache capacity (and thus 
improving hit rate) towards data that resides in slower remote NUMA zones, at the 
expense of data that resides in faster local NUMA zones. To this end, we propose 
a new NUMA-aware cache architecture that dynamically balances cache capacity based on 
memory system utilization. Before diving into microarchitectural details and results,
we first describe the locality-optimized GPU software runtime that enables our proposed
NUMA-aware architecture.

\begin{figure*}[tp] 
    \centering
    \includegraphics[width=1.0\linewidth]{figures/plot_different_baselines.pdf}
    \caption{Performance of a 4-socket NUMA GPU relative to a single GPU 
and a hypothetical 4$\times$ larger (all resources scaled) single GPU.
Applications shown in grey 
achieve greater than 99\% of performance scaling with SW-only locality optimization.}
    \label{fig:motivation}
\vspace{-.2in}
\end{figure*}
