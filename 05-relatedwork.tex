\section{Related Work}
Multi-GPU programming is commonly used for scaling GPU performance via integration 
of multiple GPUs at the system 
level~\cite{pascal-tesla-wp,dgx,intersect360,titan_supercomputer} for a 
rapidly growing pool of 
applications~\cite{coral,cudnn,Lavin15b,SimonyanZ14a}. Similarly, 
multi-socket and multi-node CPU installations have been employed and studied 
in context of high performance computing and datacenter 
applications~\cite{Intel:Xeon,IBM:Power,IBM:z196,AMD:Opteron}.
Multi-GPU programming requires explicit design for multiple GPUs
using SW APIs such as Peer-2-Peer access~\cite{NVIDIAP2P} or a combination of
MPI and CUDA~\cite{NVIDIAMPI}. These extensions require
unique programming experience and significant SW effort while adapting a
traditional GPU application to a multi-GPU system. In this paper
we execute single GPU applications on a NUMA multi-GPU system as if it was a single
larger GPU via hardware innovations and extensions to the driver software stack;
providing programmer and OS transparent execution, similarly to approaches
proposed in the past~\cite{Cabezas2015,lee2013transparent,ben2015memory}.

Modern multi-socket CPU and GPU systems leverage advanced interconnect
technologies such as NVLink, QPI and
Infinity~\cite{dgx,INTELQPI,AMDINFINITYFABRIC}. These modern fabrics utilize
high speed serial signalling technologies over unidirectional lanes
collectively comprising full-duplex links. Link capacity is
statically allocated at design time and usually is symmetric in nature. \hl{Assymetric
network on chips and NUMA interconnects have been previously investigated
and deployed}~\cite{ZiabariNOCS-15,Oracle-M7}. In this
paper we propose to dynamically re-allocate available link bandwidth resources
by using same system wiring resources and on-chip I/O interfaces, while
implementing both receiver and transmitter driver circuitry on each lane. This
approach resembles previously proposed tristate bi-directional bus
technologies~\cite{tri-state} or former technologies such as the Intel front-side
bus~\cite{fsb}, albiet with just two bus clients. However our proposal leverages fast
singled ended signalling while allowing a dynamically controlled
asymmetric bandwidth allocation via on-the-fly reconfiguration of the
individual lane direction within a link.

Static and dynamic cache partitioning techniques were widely explored in the
context of CPU caches and QoS
~\cite{ics2007,Herdrich2016CacheQF,pact06,qureshi-micro,jaleel-pact} For
example, Rafique et.~al~\cite{pact06} proposed architectural support for shared
cache management with quota-based approach. Qureshi et.~al~\cite{qureshi-micro}
proposed to partition cache space between applications. Jaleel et.~al~\cite{jaleel-pact} 
improved on this by proposing adaptive insertion
policies. Recently, cache monitoring and allocation technologies were added to
Intel Xeon processors, targeted for QoS enforcement via dynamic repartitioning
of on-chip CPU cache resources~\cite{Herdrich2016CacheQF} between applications.  
Efficient cache partitioning in the GPU has been explored in context of L1 caches
~\cite{li-priority-based} to improve application throughput. While dynamic cache 
partitioning has been widely used for
QoS and L1 utilization, to the best of our knowledge it has never been used
to try to optimize performance when caches are backed by NUMA memory
systems.
