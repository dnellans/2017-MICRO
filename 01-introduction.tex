\section{Introduction}
\label{introduction}
GPU accelerated computation has transformed high performance computing,
machine learning, and data analytics installations that were previously
dominated by CPUs~\cite{pascal,intersect360,cudnn,Lavin15b,SimonyanZ14a}.
These systems now combine GPUs and CPUs to leverage high throughput data parallel
computations (GPUs) in combination with the traditional benefits of latency critical
serial execution (CPUs).  While performance is paramount, GPU computing has been 
successful in these domains because of its strong support for 
data parallel programming languages~\cite{CUDA7,OPENCL} reduce programmer burden
when trying to scale their programs across every growing data sets.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{figures/inter_gpu_connections.pdf}
\caption{Evolution of GPUs to switch connected multi-socket GPU.}
\label{fig:systemdiagram}
\vspace{-.15in}
\end{figure}

While GPU programming models support explicit programming of multiple GPUs, the 
interfaces for this are a departure from the straight forward single GPU 
programming model, requiring systems like Peer-2-Peer access~\cite{NVIDIAP2P} 
or a combination of MPI + CUDA~\cite{NVIDIAMPI} to manage multiple GPUs.  These 
programming extensions enable advanced programmers to leverage more than one 
GPU for high throughput computation but require re-writing of the GPU 
application, a major barrier to adoption.  With GPUs nearing the reticle limit 
for maximal die size and transistor density growth slowing down, programmers 
looking to scale the performance of their single GPU program are in a precarious 
position.

Recently, GPUs have started making the transition from using the PCIe peripheral 
interface
to having improved interconnections between both the GPUs and 
CPUs~\cite{dgx,AMDINFINITYFABRIC}
as shown in Figure~\ref{fig:systemdiagram}.  GPUs now blur the line between 
being
a discrete pluggable PCIe peripheral and being an design time component using a 
high pin-count
socket interface similar to current CPU interfaces.  This socket interface is required
not only for power delivery, but because the GPU and CPU interconnects require printed
circuit board (PCB) level integration to high 10's to 100's of GB/s of interconnect bandwidth.
This evolution of GPUs from single socket, to high connectivity multi-socket designs
is a natural progression when trying to improve intra-GPU communication.

Multi-socket GPUs provide a pivot point for GPU vendors.  In one direction, they
could continue to expose multi-socket GPUs as discrete GPUs and force programmers
to re-write their applications to try and improve scaling beyond a single socket.
In the other, they could now choose to expose these multi-socket designs as asymmetric
non-uniform memory access (NUMA) GPUs.  We believe the later solution is more likely
because it allows significant performance scaling (2-8 GPUs in a single node) beyond
that which is achievable through the evolution of single GPU transistor growth, all
while maintaining the programming model GPU developers have standardized upon.

We are not the first group to propose that there may be potential performance
benefits to aggregating GPUs together, without changing programming 
interface~\cite{Cabezas2015,lee2013transparent,XXX,XXX}. However, much of this work
was done in an era where GPUs were limited in the memory they could address
in the system and high latency PCIe interconnects were the standard interconnect
technology.  These restrictions cause prior work to primarily solve programming and runtime
interface issues when trying to aggregate multiple discrete GPUs.  We propose
that in the era of unified virtual addressing, cache line addressable high bandwidth
interconnects, and dedicated CPU--GPU socketed PCB designs, multi-socket GPUs
should be treated as asymmetric NUMA-GPUs and their microarchitectures need to be optimized
to maximize multi-socket GPU performance.

In this work we examine future an evolutionary multi-socket GPU system and describe the 
basic modifications needed to allow this multi-socket GPU system to appear to the programmer
as single large GPU, obeying all of the current GPU programming paradigms. We apply 
several well known NUMA techniques from the CPU world to this system and show that
while having a positive effect, simply applying CPU NUMA design principles to GPUs 
will be insufficient to achieve scalable performance.  Thus we propose that to design
single program scalable multi-GPU systems GPU architectures themselves must become
\textit{asymmetry-aware} to handle the NUMA effects that are implicit in a multi-socket
design.

In this work, we make the following contributions:

\begin{enumerate}
\item
We show that despite being highly data parallel, GPU programs have significant
dynamic phase behavior both between kernels and among CTAs within a single
kernel that should be microarchitecturally exploited to eliminate observed
NUMA effects.

\item
We show that traditional 50--50 splits of (up and 
downstream) GPU--GPU interconnect bandwidth limits multi-GPU scalability.  We propose
that GPU interconnects need to support dynamic bandwidth balancing that can
turn around $N$ of $M$ high speed serial links at run time to support asymmetric
bandwidth requirements.  We demonstrate that our dynamic policy is coarse
grained enough that the performance improvement is largely insensitive to link turn 
around and policy sample time.  We show that dynamic policies must be made on a
per GPU basis, global observation and management fails to capture asymmetry 
within the system.

\item
We show that the current GPUs which utilize a SW based coherency among L1's and 
memory-side L2 caches will not perform well in a multi-socket scenario.  
We examine the cost and trade-offs of extending such a coherency scheme into L2 caches 
which enables multi-socket GPU cache coherency within a node.  We examine several
caching policies that balance the effect of caching efficiency with coherence overheads
and show that without modification, current GPU coherence protocols may perform
well in properly architected multi-GPU caches.

\item
We show that architecting a single static cache architecture for multi-GPUs will
be sub-optimal for performance. We show that balancing cache capacity between
local and remote accesses in highly NUMA systems is required both on a per
workload basis, but also a per socket basic to leverage application phase
behavior.  By allowing cache capacity to be dynamically optimized to compensate for
bandwidth saturation, caches can help hide NUMA effects in multi-socket GPUs.

% Dynamic cache partitioning: given that different benchmarks prefer different 
% cache configurations, we evaluate dynamic way-partitioning of gpu-side L2 
% caches. Depending on local and remote BW saturations, we adjust the number of 
% ways reserved for caching local and remote data. Alone, this mechanism improves 
% the performance by X% and we are within Y% of 1x gpu-side RC + 1x mem-side L2. 
% Combined with dynamic NVLink balancing, we improve the performance by Z% over 
% the baseline TMG and we are within ZZ% of 2x NVlink + 1x gpu-side + 1x mem-side 
% L2. Finally, this increases the scalability compared to a single-GPU system by 
% YY%.




% 
% General story about increasing the performance by putting multiple GPUs working 
% together (TMG). No code re-writing, no re-compilation, what can we expect if we 
% just allow the runtime to distribute CTAs across multiple GPUs?
% With CUDA providing UVAS and UMA, and NVLink supporting atomic , GPU-NUMA system 
% is born: there is ~10x difference between local and remote BW (local mem 
% BW=1TB/s and NVLink BW is 128GB/s) which limits the scalability and stands as a 
% bottleneck. It is not about latency, it is about BW. (maybe some nvlink latency 
% vs bw graph here?). The question is what can we do from the micro-arch side to 
% improve this?

% Technology trends indicate an increasing number of systems designed with CPUs, 
% accelerators, and GPUs coupled via high-speed 
% links. Such systems are likely to introduce unified shared
% CPU-GPU memory with shared page tables. In fact, some systems already
% feature such implementations~\cite{AMDKaveri}.
% Introducing globally visible shared memory
% improves programmer productivity by eliminating explicit copies and memory 
% management overheads. Whereas this abstraction can be supported using
% software-only page-level protection mechanisms~\cite{UVM, HSA}, hardware cache coherence 
% can improve performance by allowing concurrent, fine-grained access to memory
% by both CPU and GPU.  If the CPU and GPU have separate physical
% memories, page migration may also be used to optimize page placement for
% latency or bandwidth by using both near and far 
% memory~\cite{Dashti2013,Agarwal2015b,Meswani2015,Chou2015}.
% 
% 
% 
% Some CPU--GPU systems will be tightly integrated into a system on chip (SoC) making on-chip 
% hardware coherence a natural fit, possibly even by sharing a portion of the on-chip 
% cache hierarchy~\cite{HSA,AMDAPU,Hechtman2014}.  However, the largest GPU 
% implementations consume nearly 8B transistors and have their own 
% specialized memory systems~\cite{NVIDIA8BILLION}.  
% Power and thermal constraints preclude single-die integration of such designs. 
% Thus, many CPU--GPU systems are likely to have 
% discrete CPUs and GPUs connected via dedicated off-chip interconnects like 
% NVLINK (NVIDIA), CAPI (IBM), HT (AMD), and QPI (INTEL) or implemented as 
% multi-chip modules~\cite{NVLINK,CAPI,AMDHT,INTELQPI,Chen92}. The availability of these
% high speed off-chip interconnects has led both academic groups and vendors like NVIDIA
% to investigate how future GPUs may integrate into existing OS-controlled 
% unified shared memory regimes used by CPUs~\cite{Pichai2014,power2014,Agarwal2015,Agarwal2015b}.
% 
% Current CPUs have up to 18 cores per socket~\cite{INTELXEONE5V3} but GPUs are 
% expected to have hundreds of streaming multiprocessors (SMs) each with its own cache(s) within 
% the next few years. Hence, extending traditional hardware cache-coherency into a multi-chip 
% CPU--GPU memory system requires coherence messages to be exchanged not just within the GPU but
% over the CPU--GPU interconnect. Keeping these hundreds of caches coherent with a traditional HW
% coherence protocol, as shown in
% Figure~\ref{fig:motivation}, potentially requires large state and interconnect 
% bandwidth~\cite{Kelm2010,johnson2011}. 
% Some recent proposals call for heterogeneous race-free (HRF) GPU programming models, which
% allow relaxed or scoped memory consistency to reduce the frequency or hide the
% latency of enforcing coherence~\cite{Hechtman2014}. 
% Others argue that scopes substantially increase programmer burden, and instead propose
% a data-race-free programming model with coherence based on reader-initiated invalidation~\cite{Sinclair2015}.
% However, irrespective of 
% memory ordering requirements, such approaches either require software to initiate flushes
% at synchronization points or system-wide hardware cache coherence mechanisms.
% We show
% that Selective Caching can achieve performance rivaling
% more complex CPU-GPU cache coherence protocols.
% Techniques like region coherence~\cite{Power2013} 
% seek to scale coherence protocols for heterogeneous systems, but require
% pervasive changes throughout the CPU and GPU memory systems. Such approaches
% also incur highly coordinated design and verification effort by both CPU and GPU
% vendors~\cite{Hong2012} that is challenging when multiple vendors wish to integrate
% existing CPU and GPU designs in a timely manner.
% 
% In the past, NVIDIA has investigated extending hardware cache-coherence 
% mechanisms to multi-chip CPU--GPU memory systems. Due to the significant challenges
% associated with building such systems, in this work, we architect a GPU \textit{selective caching} 
% mechanism. This mechanism provides the conceptual simplicity of CPU--GPU hardware cache coherence and
% maintains a high level of GPU performance, but does not actually implement
% hardware cache coherence within the GPU, or between the CPU and GPU. In our proposed 
% selective caching GPU, the GPU does not cache data that resides in CPU physical 
% memory, nor does it cache data that resides in the GPU memory that is 
% actively in-use by the CPU on-chip caches. This approach is orthogonal to the memory
% consistency model and leverages the latency-tolerant nature of GPU architectures combined with upcoming low-latency and 
% high-bandwidth interconnects to enable the benefits of shared memory.  To evaluate the performance
% of such a GPU, we measure ourselves against a theoretical hardware
% cache-coherent CPU--GPU system that, 
% while high performance, is impractical to implement.
% 
% In this work, we make the following contributions:
% 
% \begin{enumerate}
% % \vspace{-.05in}
% \item
% We propose GPU selective caching, which enables a CPU--GPU system that provides a 
% unified shared memory without requiring hardware cache-coherence protocols within the GPU
% or between CPU and GPU caches.
% % \vspace{-.05in}
% \item
% We identify that much of the improvement from GPU caches is due to coalescing 
% memory accesses that are spatially contiguous.  Leveraging
% aggressive request coalescing, GPUs can achieve much of the performance benefit
% of caching, without caches.
% % \vspace{-.05in}
% \item
% We propose a small on-die CPU cache to handle uncached GPU requests
% that are issued at sub-cache line granularity. This cache helps both 
% shield the CPU memory system from the bandwidth hungry GPU and supports
% improved CPU--GPU interconnect efficiency by implementing variable-sized transfer granularity.
% % \vspace{-.05in}
% \item
% We demonstrate that a large fraction of GPU-accessed data is read-only. Allowing 
% the GPU to cache this data and relying on page protection mechanisms rather than hardware 
% coherence to ensure correctness closes the performance gap between a selective
% caching and hardware cache-coherent GPU for many applications.
% % \vspace{-.05in}
\end{enumerate}
