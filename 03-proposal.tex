\section{A NUMA-Aware GPU Runtime}

Current GPU software and hardware is co-designed to optimize 
throughput based on the assumption of uniform memory properties 
within the GPU. Fine grained interleaving of addresses across
memory channels provides implicit load balancing but destroys 
locality.  As a result thread block (CTA) scheduling policies need not 
be sophisticated to try and capture locality, which has already been
destroyed.  For future NUMA GPUs to work well both the GPU runtime layer and 
hardware must be changed to achieve scalable performance.

Prior work has demonstrated it is possible to design a framework and runtime 
system that transparently decomposes GPU kernels in sub-kernels and executes 
them on multiple PCIe attached GPUs in parallel~\cite{Cabezas2015}. For example, on NVIDIA GPUs 
this can be implemented by intercepting and remapping each kernel call, GPU 
memory allocation, memory copy, and GPU-wide synchronization issued by the CUDA 
driver. Special care is needed to ensure that per-GPU memory fences are promoted to 
the system level and seen by all GPUs, as well as guaranteeing that sub-kernel CTA 
identifiers are properly managed to reflect those of the original kernel. Cabezas
et. al solve these two problems by introducing source code 
annotations and an additional source-to-source compiler which is also 
responsible for statically partitioning data placement and computation, breaking
our goal of programmer transparency.

In this work, we follow a similar strategy but without using an source-to-source
translation or code annotation. Instead, we propose relying on NVIDIA's Unified Virtual 
Addressing~\cite{UVM} to allow dynamic placement of pages into memory at
runtime rather than via compiler driven memory placement.  Similarly, technologies with
cache line granularity interconnects like NVIDIA's NVLink~\cite{NVLINK} allow
transparent access to remote memory without the need to modify application
source code when accessing local or remote memory addresses that are dynamically
discovered at runtime. With these two enabling technologies, the remaining code
transformations required to execute a single kernel on multiple sub-GPUs can
be performed during the dynamic compilation of PTX to SASS at runtime with no program
changes.  As such, our multi-GPU runtime need only statically identify and promote 
system wide memory fences and manage sub-kernel CTA identifiers, launches, and completions.

On a single GPU fine grain dynamic assignment of CTAs to SMs is utilized to 
achieve good load balancing.  Extending this policy to a multi-socket GPU 
system is not possible due to the high latency of passing sub-kernel 
launches from software to hardware.  To amortize this cost the multi-GPU runtime 
must launch a block of CTAs to each GPU-socket at coarse granularity.  To 
encourage load balancing, each sub-kernel could be comprised of an interleaving 
of the overall CTAs using modulo arithmetic.  Alternatively a single kernel can be 
decomposed into N sub-kernels, where N is the total number of GPU sockets in 
the system, assigning equal amount of contiguous CTAs to each GPU.  This design 
potentially exposes workload imbalance across sub-kernels, but it has 
been shown to preserve data locality present in applications where contiguous 
CTAs also access contiguous memory regions~\cite{Cabezas2015,Arunkumar2017}.

Contiguous CTA launches will improve NUMA locality only if the memory they access
also has physical locality. Current GPUs perform fine grained memory interleaving 
at a sub-page granularity 
across memory channels.  In a NUMA GPU this policy destroys locality and 
result in 75\% of all accesses to be to remote memory in a 4 GPU system. Similarly, a round-robin page level 
interleaving could be utilized, similar to the Linux INTERLEAVE page allocation 
strategy, but this still will results in 
75\% (on average) of memory accesses occurring over low bandwidth NUMA links. To
try and optimized memory placement and locality we leverage NVIDIA's 
UVM page migration functionality to determine the page-level memory placement
at GPU run time. The UVM memory system effectively migrate pages on-demand from system 
memory to specific GPU-socket memory upon first access (also called first-touch 
allocation) as described by Arunkumar et. al~\cite{Arunkumar2017}. In
our NUMA GPU system this results in memory physically residing on the GPU-socket
in conjunction with the CTA that first accessed it, improving locality but
potentially hurting memory load balancing.

\vspace{-.05in}
\subsection{SW Locality-Optimized Performance}

To understand the impact software level decisions can have on NUMA GPU
performance, Figure~\ref{fig:motivation} shows the relative performance of a 4-socket NUMA 
GPU with respect to a single GPU under the two possible CTA scheduling and 
memory placement strategies.  The green bars show the 
relative performance of traditional single GPU scheduling and memory 
interleaving policies when adapted to a NUMA GPU. The light blue bars
show the relative performance of using locality optimized GPU scheduling and 
memory placement, consisting of contiguous block CTA scheduling and first touch 
page migration. We observe that the \textit{Locality-Optimized} 
solution almost always outperforms traditional GPU scheduling and memory 
interleaving. Without these runtime locality optimizations, on average a 4-socket NUMA 
GPU is not able to match the performance of a single GPU despite the large 
increase in hardware resources. As a result, we now only consider this locality 
optimized GPU runtime for the remainder of this work.

Despite the improvements achieved via software locality optimization, many applications do 
not scaling well on a 4-socket NUMA GPU. To illustrate this, Figure~\ref{fig:motivation} 
also shows the application performance achievable by a hypothetical (and unbuildable)
4$\times$ larger GPU with proportionally scaled resources. This hypothetical GPU
represents the maximum performance we could expect from a perfectly architected 
(both HW and SW) NUMA GPU system. 

Sorted by the differential in hypothetical to achieved performance, some 
workloads (shown in the grey box) already achieve near
maximum performance. For the two right-most 
benchmarks, the SW locality optimized NUMA GPU can even outperform the 
hypothetical 4x larger GPU due higher cache hit rates caused by cache friendly
contiguous block scheduling. For many applications there is still a large differential between the 
locality optimized NUMA design and theoretical performance. These are workloads 
in which either locality does not exist or the locality optimized GPU runtime is 
ineffective. Because our goal is to provide scalable performance for all single GPU 
optimized applications, in the rest of the paper we aim to close this 
performance gap through microarchitectural innovation. To simplify later 
discussion, we choose to exclude benchmarks that can already achieve $\geq$99\% of the 
theoretical performance with SW-only locality optimizations. However, we include 
all benchmarks in our final results showing the overall projected scalability 
of NUMA-aware multi-socket GPUs.
\vspace{-.05in}
\subsection{Simulation Methodology}
\label{sec:methodology}

To evaluate the performance of future NUMA-aware multi-socket GPUs we use a 
proprietary, cycle-level, trace-driven simulator for single and multi-GPU 
systems. Our baseline GPU in both single GPU and multi-socket GPU 
configurations approximates the latest NVIDIA Pascal 
architecture~\cite{pascal-tesla-wp}. Each streaming multiprocessor (SM) is 
modeled as an in-order processor with multiple levels of cache hierarchy
containing private, per-SM, L1 caches and a banked, shared, L2 cache. Each 
GPU-socket is backed by on-package high bandwidth memory~\cite{HBM}. Our 
multi-socket GPU systems contains two to eight of these GPUs interconnected 
through a full bandwidth GPU switch as shown in Figure~\ref{fig:systemdiagram}. 
Table~\ref{tab:setup} provides a more detailed overview of the simulation 
parameters.

We study the scalability of multi-socket NUMA GPUs using 41 workloads 
taken from a range of 
production codes based on the HPC CORAL benchmarks~\cite{coral}, graph 
applications from Lonestar~\cite{lonestar}, HPC applications from 
Rodinia~\cite{Che2009}, in addition to several other in-house CUDA benchmarks. 
This set of workloads covers a wide spectrum of GPU applications used in 
machine learning, fluid dynamic, image manipulation, graph traversal, and
scientific computing.  Each of our benchmarks is hand selected to identify
the region of interest deemed representative for each workload, which may
be as small as a single kernel containing a tight inner loop or several thousand
kernel invocations. Table~\ref{tab:numctas} provides both the memory footprint
of these workloads as well as the average number of active CTAs in the workload
(weighted by the time spent on each kernel), providing a representation of 
how many parallel thread blocks (CTAs) are available during workload execution.

\begin{table}[tp]
\begin{small}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value(s)} \\
\toprule
Num of GPU sockets & 4 \\
\midrule
Total number of SMs & 64 per GPU socket \\
\midrule
GPU Frequency & 1GHz \\
\midrule
Max number of Warps & 64 per SM \\
\midrule
Warp Scheduler & Greedy then Round Robin \\
\midrule
L1 Cache & Private, 128 KB per SM, 128B lines, 4-way, \\ 
& GPU-side SW-based coherency \\
\midrule
L2 Cache & Shared, 4MB per socket, 128B lines, 16-way, \\ 
& Memory-side non-coherent\\
\midrule
GPU--GPU Interconnect & 128GB/s per socket (64GB/s each direction) \\
& 8 lanes 8B wide each per direction \\
&128-cycle latency \\
\midrule
DRAM Bandwidth & 768GB/s per GPU socket\\
\midrule
DRAM Latency & 100ns \\
\toprule
\end{tabular}
\vspace{-.1in}
\caption{Simulation parameters for evaluation of single and multi-socket GPU 
systems.}
\vspace{-.2in}
\label{tab:setup}
\end{small}
\end{table} 


\begin{table}[t]
\begin{small}
\centering
\begin{tabular}{lcc}
 \toprule
 \textbf{Benchmark} & \textbf{Time-weighted} & 
\textbf{Memory} \\
& \textbf{Average CTAs} & \textbf{Footprint (MB)} \\
 \toprule
ML-GoogLeNet-cudnn-Lev2 & 6272 & 1205 \\
ML-AlexNet-cudnn-Lev2 & 1250 & 832 \\
ML-OverFeat-cudann-Lev3 & 1800 & 388 \\
ML-AlexNet-cudnn-Lev4 & 1014 & 32 \\
ML-AlexNet-ConvNet2 & 6075 & 97 \\
Rodinia-Backprop & 4096 & 160 \\
Rodinia-Euler3D & 1008 & 25 \\
Rodinia-BFS  & 1954 & 38 \\
Rodinia-Gaussian  & 2599 & 78 \\
Rodinia-Hotspot  & 7396 & 64 \\
Rodinia-Kmeans  & 3249 & 221 \\
Rodnia-Pathfinder  & 4630 & 1570 \\
Rodinia-Srad  & 16384 & 98 \\
HPC-SNAP  & 200 & 744 \\
HPC-Nekbone-Large & 5583 & 294 \\
HPC-MiniAMR & 76033 & 2752 \\
HPC-MiniContact-Mesh1  & 250 & 21 \\
HPC-MiniContact-Mesh2  & 15423 & 257 \\
HPC-Lulesh-Unstruct-Mesh1  & 435 & 19 \\
HPC-Lulesh-Unstruct-Mesh2  & 4940 & 208 \\
HPC-AMG & 241549 & 3744 \\
HPC-RSBench  & 7813 & 19 \\
HPC-MCB  & 5001 & 162 \\
HPC-NAMD2.9  & 3888 & 88 \\
HPC-RabbitCT  & 131072 & 524 \\
HPC-Lulesh  & 12202 & 578 \\
HPC-CoMD  & 3588 & 319 \\
HPC-CoMD-Wa  & 13691 & 393 \\
HPC-CoMD-Ta  & 5724 & 394 \\
HPC-HPGMG-UVM  & 10436 & 1975 \\
HPC-HPGMG  & 10506 & 1571 \\
%HPC-HPGMG-UVM-Base & 359 & 10728 & 1975 \\
Lonestar-SP & 75 & 8 \\
Lonestar-MST-Graph & 770 & 86 \\
Lonestar-MST-Mesh & 895 & 75 \\
%HPC-Nekbone-medium & 510 & 3093 & 170 \\
Lonestar-SSSP-Wln & 60 & 21 \\
Lonestar-DMR  & 82 & 248 \\
Lonestar-SSSP-Wlc  & 163 & 21 \\
Lonestar-SSSP  & 1046 & 38 \\
Other-Stream-Triad  & 699051 & 3146 \\
Other-Optix-Raytracing  & 3072 & 87 \\
Other-Bitcoin-Crypto  & 60 & 5898 \\
\toprule
\end{tabular}
\vspace{-.1in}
\caption{Application footprint and average number of CTAs (thread blocks) 
available during time-weighted execution.}
\vspace{-.2in}
\label{tab:numctas}
\end{small}
\end{table}


% \begin{table}[t]
% \begin{small}
% \centering
% \begin{tabular}{lccc}
%  \toprule
%  \textbf{Benchmark} & \textbf{Kernels} & \textbf{Time-weighted} & 
% \textbf{Memory} \\
% & & \textbf{Average CTAs} & \textbf{(MB)} \\
%  \toprule
% ML-GoogLeNet-cudnn-Lev2 & 1 & 6272 & 1205 \\
% ML-AlexNet-cudnn-Lev2 & 1 & 1250 & 832 \\
% ML-OverFeat-cudann-Lev3 & 1 & 1800 & 388 \\
% ML-AlexNet-cudnn-Lev4 & 1 & 1014 & 32 \\
% ML-AlexNet-ConvNet2 & 1 & 6075 & 97 \\
% Rodinia-Backprop & 2 & 4096 & 160 \\
% Rodinia-Euler3D & 346 & 1008 & 25 \\
% Rodinia-BFS & 24 & 1954 & 38 \\
% Rodinia-Gaussian & 510 & 2599 & 78 \\
% Rodinia-Hotspot & 1 & 7396 & 64 \\
% Rodinia-Kmeans & 3 & 3249 & 221 \\
% Rodnia-Pathfinder & 20 & 4630 & 1570 \\
% Rodinia-Srad & 4 & 16384 & 98 \\
% HPC-SNAP & 118 & 200 & 744 \\
% HPC-Nekbone-Large & 300 & 5583 & 294 \\
% HPC-MiniAMR & 33 & 76033 & 2752 \\
% HPC-MiniContact-Mesh1 & 500 & 250 & 21 \\
% HPC-MiniContact-Mesh2 & 127 & 15423 & 257 \\
% HPC-Lulesh-Unstruct-Mesh1 & 2000 & 435 & 19 \\
% HPC-Lulesh-Unstruct-Mesh2 & 200 & 4940 & 208 \\
% HPC-AMG & 88 & 241549 & 3744 \\
% HPC-RSBench & 1 & 7813 & 19 \\
% HPC-MCB & 1 & 5001 & 162 \\
% HPC-NAMD2.9 & 1 & 3888 & 88 \\
% HPC-RabbitCT & 1 & 131072 & 524 \\
% HPC-Lulesh & 105 & 12202 & 578 \\
% HPC-CoMD & 350 & 3588 & 319 \\
% HPC-CoMD-Wa & 350 & 13691 & 393 \\
% HPC-CoMD-Ta & 350 & 5724 & 394 \\
% HPC-HPGMG-UVM & 359 & 10436 & 1975 \\
% HPC-HPGMG & 317 & 10506 & 1571 \\
% HPC-HPGMG-UVM-Base & 359 & 10728 & 1975 \\
% Lonestar-SP & 11 & 75 & 8 \\
% Lonestar-MST-Graph & 87 & 770 & 86 \\
% Lonestar-MST-Mesh & 71 & 895 & 75 \\
% HPC-Nekbone-medium & 510 & 3093 & 170 \\
% Lonestar-SSSP-Wln & 1000 & 60 & 21 \\
% Lonestar-DMR & 3 & 82 & 248 \\
% Lonestar-SSSP-Wlc & 1300 & 163 & 21 \\
% Lonestar-SSSP & 102 & 1046 & 38 \\
% Other-Stream-Triad & 5 & 699051 & 3146 \\
% Other-Optix-Raytracing & 1 & 3072 & 87 \\
% Other-Bitcoin-Crypto & 1 & 60 & 5898 \\
% \toprule
% \end{tabular}
% \caption{Application footprint and average number of CTAs (thread blocks) 
% available during time-weighted execution.}
% \label{tab:numctas}
% \end{small}
% \end{table}
