\section{Asymmetry Aware GPUs}

Start with meat of the proposals that we're going to talk about two big things, nvlink and then caching
But first we'll talk about our modeling methodology


\subsection{Methodology}
\label{methodology}

%Put all our methodology here and update the table for our simulation infra

To evaluate the performance of multi-socket GPUs we use a proprietary, 
cycle-level, trace-driven simulator for GPU systems running CUDA applications. 
For our baseline, we model a single GPU according to the latest NVIDIA's Pascal 
GPU~\cite{inside-pascal}. Each of the Streaming Multiprocessors is modeled as an 
in-order processor with multiple levels of cache hierarchy containing private, 
per-SM, L1 caches and multi-banked, shared, L2 cache. Each GPU is backed by its 
local high bandwidth DRAM memory. Our multi-socket GPU system contains four of 
these GPUs interconnected through a full bandwidth GPU switch as shown on 
Figure~\ref{}. Table~\ref{tab:setup} stands as an overview of the simulation 
parameters.

\begin{table}[tp]
\begin{small}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value(s)} \\
\toprule
Num of GPU sockets & 4 \\
\midrule
Total number of SMs & 64 per GPU socket \\
\midrule
GPU Frequency & 1GHz \\
\midrule
Max number of Warps & 64 per SM \\
\midrule
Warp Scheduler & Greedy then Round Robin \\
\midrule
L1 Cache & Private, 128 KB per SM, 128B lines, 4 ways, \\ & GPU-side SW-based coherency \\
\midrule
L2 Cache & Shared, 4MB per 64 SM, 128B lines, 16 ways, \\ & Memory-side non-coherent\\
\midrule
Inter-GPU Interconnect & 128GB/s per socket, 8 lanes 8B wide each per \\ & direction, 128-cycle latency \\
\midrule
DRAM Bandwidth & 768GB/s per GPU socket\\
\midrule
DRAM Latency & 100ns \\
\toprule
\end{tabular}
\caption{Simulation parameters for evaluation of single and multi-socket GPU systems.}
\label{tab:setup}
\end{small}
\end{table}

We study our proposal using NN benchmarks taken from a broad range of production 
codes based on the HPC CORAL benchmarks~\cite{coral}, graph applications from 
Lonestar~\cite{lonestar}, compute applications from Rodinia~\cite{Che2009}, in 
addition to several other in-house CUDA benchmarks. This set of workloads covers 
a wide spectrum of GPU applications used in machine learning, fluid dynamic, 
image manipulation, graph traversal, scientific computing, etc. We run our 
simulations until the completion of the entire applications or the number of 
kernels shown on Table~\ref{tab:numctas}.


