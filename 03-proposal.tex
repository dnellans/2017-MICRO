\section{NUMA-Aware GPUs}

As shown in Section~\ref{background}, creating larger multi-socket GPUs that 
perform well across all the benchmarks is not trivial despite the 
improvements that technologies like NVLink~\cite{NVLINK} and Unified Virtual 
Addressing~\cite{UVM} can provide. While the most basic multi-socket GPU 
implementation may extend current GPU thread block scheduling and memory 
management techniques across sockets, this results in sub-optimal 
performance.  As shown in Figure~\ref{fig:motivation2}, even applying NUMA 
optimization techniques in the multi-socket GPU runtime such as first touch 
page placement and Block CTA partitioning leaves significant performance on 
the table compared to an hypothetical 4x larger (all resources scaled) GPU on many 
benchmarks.

To improve the performance of our baseline multi-socket GPU implementation we 
propose two classes of improvements that leverage application phasing to reduced 
the effective NUMA penalty.  First, in 
Section~\ref{interconnect} we examine the ability of a switch connected GPU to 
dynamically change its interconnect signaling from a balanced up and down 
stream bandwidth design, to allowing flexible re-partitioning of these channels.  
When bi-directional bandwidth is observed to be under-utilized, the direction 
which has excess capacity can be re-assigned to support the oversubscribed 
channels. This allows any individual GPU to improve its bandwidth utilization at 
times when it finds itself most bandwidth constrained.

Second, because we observe that inter-GPU bandwidth has a strong correlation to 
overall multi-socket GPU performance we investigate the performance impact of 
enabling multi-socket GPU cache coherence in Section~\ref{caching}.  We propose 
extending the software based L1 coherence into the L2 caches of multi-socket 
GPUs.  By extending this coherence, the L2 caches of each GPU socket move from 
being memory-side, where coherence is not needed, to GPU-side, where coherence 
guarantees are required in the same manner as current GPU L1 caches.  We 
evaluate the effect of this coherence overhead on multi-socket GPUs and show 
that because of the GPU's weak coherence guarantees multi-socket coherence on 
GPUs should scale significantly better than traditional CPU coherence protocols.

Finally, we show that after extending coherence into the GPU L2 caches, the 
appropriate allocation of cache capacity between local memory bandwidth and 
remote memory bandwidth can not be decided at design time.  Due to the same 
application phasing that enabled dynamic interconnect rebalancing, we observe 
that individual GPU sockets should be free to balance their cache capacity 
between local and remote accesses to optimize performance.  When remote 
interconnect bandwidth is saturated, a GPU will skew its cache capacity towards 
the remote accesses,  however if that bandwidth is not saturated then optimizing 
cache hit rates to eliminate the largest number of memory requests is 
desirable. Before diving into detailed proposals and results for each of these 
optimizations we first describe our simulations methodology.

\begin{table}[tp]
\begin{small}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value(s)} \\
\toprule
Num of GPU sockets & 4 \\
\midrule
Total number of SMs & 64 per GPU socket \\
\midrule
GPU Frequency & 1GHz \\
\midrule
Max number of Warps & 64 per SM \\
\midrule
Warp Scheduler & Greedy then Round Robin \\
\midrule
L1 Cache & Private, 128 KB per SM, 128B lines, 4-way, \\ 
& GPU-side SW-based coherency \\
\midrule
L2 Cache & Shared, 4MB per socket, 128B lines, 16-way, \\ 
& Memory-side non-coherent\\
\midrule
GPU--GPU Interconnect & 128GB/s per socket (64GB/s each direction) \\
& 8 lanes 8B wide each per direction \\
&128-cycle latency \\
\midrule
DRAM Bandwidth & 768GB/s per GPU socket\\
\midrule
DRAM Latency & 100ns \\
\toprule
\end{tabular}
\caption{Simulation parameters for evaluation of single and multi-socket GPU 
systems.}
\label{tab:setup}
\end{small}
\end{table}

\subsection{Methodology}
\label{methodology}
 To evaluate the performance of multi-socket GPUs we use a proprietary, 
cycle-level, trace-driven simulator for GPU systems running single GPU CUDA 
applications. For our baseline, we model a single GPU that approximates the 
latest NVIDIA Pascal architecture~\cite{inside-pascal}. Each of the Streaming 
Multiprocessors (SM) is modeled as an in-order processor with multiple levels of cache hierarchy 
containing private, per-SM, L1 caches and multi-banked, shared, L2 cache. Each 
GPU is backed by its local high bandwidth DRAM memory. Our multi-socket GPU 
system contains four of these GPUs interconnected through a full bandwidth GPU 
switch as shown on Figure~\ref{fig:systemdiagram}. Table~\ref{tab:setup} stands as an overview of the simulation parameters.

We study our proposal using XXX benchmarks taken from a broad range of 
production codes based on the HPC CORAL benchmarks~\cite{coral}, graph 
applications from Lonestar~\cite{lonestar}, compute applications from 
Rodinia~\cite{Che2009}, in addition to several other in-house CUDA benchmarks. 
This set of workloads covers a wide spectrum of GPU applications used in machine 
learning, fluid dynamic, image manipulation, graph traversal, scientific 
computing, etc. We run our simulations until the completion of the entire 
applications or the number of kernels shown on Table~\ref{tab:numctas}.

\begin{table}[t]
\begin{small}
\centering
\begin{tabular}{lccc}
 \toprule
 \textbf{Benchmark} & \textbf{Kernels} & \textbf{Time-weighted} & \textbf{Memory} \\
& & \textbf{Average CTAs} & \textbf{(MB)} \\
 \toprule
ML-GoogLeNet-cudnn-L2 & 1 & 6272 & 1205 \\
ML-AlexNet-cudnn-L2 & 1 & 1250 & 832 \\
Optix-Raytracing & 1 & 3072 & 87 \\
Bitcoin-Crypto & 1 & 60 & 5898 \\
ML-OverFeat-cudann-L3 & 1 & 1800 & 388 \\
%Encription & 1 & 128 & 1 \\
ML-AlexNet-cudnn-L4 & 1 & 1014 & 32 \\
ML-AlexNet-ConvNet2 & 1 & 6075 & 97 \\
HPC-Namd2.9 & 1 & 3888 & 88 \\
HPC-RabbitCT & 1 & 131072 & 524 \\
Rodinia-Backprop & 2 & 4096 & 160 \\
Rodinia-Euler3D & 346 & 1008 & 25 \\
Rodinia-BFS & 24 & 1954 & 38 \\
Rodinia-Gaussian & 510 & 2599 & 78 \\
Rodinia-Hotspot & 1 & 7396 & 64 \\
Rodinia-Kmeans & 3 & 3249 & 221 \\
Rodnia-Pathfinder & 20 & 4630 & 1570 \\
Rodinia-Srad & 4 & 16384 & 98 \\
Lonestar-SP & 11 & 75 & 8 \\
HPC-Lulesh & 105 & 12202 & 578 \\
Lonestar-MST-graph & 87 & 770 & 86 \\
Lonestar-MST-mesh & 71 & 895 & 75 \\
HPC-CoMD & 350 & 3588 & 319 \\
HPC-CoMD-wa & 350 & 13691 & 393 \\
HPC-CoMD-ta & 350 & 5724 & 394 \\
HPC-HPGMG-UVM-opt & 359 & 10436 & 1975 \\
HPC-HPGMG & 317 & 10506 & 1571 \\
%HPC-HPGMG-UVM-base & 359 & 10728 & 1975 \\
HPC-Lulesh-Unstruct-small & 2000 & 435 & 19 \\
%HPC-Nekbone-medium & 510 & 3093 & 170 \\
Lonestar-SSSP-wln & 1000 & 60 & 21 \\
HPC-SNAP & 118 & 200 & 744 \\
HPC-Nekbone-large & 300 & 5583 & 294 \\
HPC-MiniAMR & 33 & 76033 & 2752 \\
HPC-MiniContact-large & 127 & 15423 & 257 \\
HPC-MiniContact-small & 500 & 250 & 21 \\
HPC-Lulesh-Unstruct-large & 200 & 4940 & 208 \\
HPC-AMG & 88 & 241549 & 3744 \\
HPC-RSBench & 1 & 7813 & 19 \\
HPC-MCB & 1 & 5001 & 162 \\
Lonestar-DMR & 3 & 82 & 248 \\
Lonestar-SSSP-wlc & 1300 & 163 & 21 \\
Lonestar-SSSP & 102 & 1046 & 38 \\
Stream-Triad & 5 & 699051 & 3146 \\
\toprule
\end{tabular}
\caption{Application footprint and average number of CTAs (threadblocks) available during time-weighted execution.}
\label{tab:numctas}
\end{small}
\end{table}