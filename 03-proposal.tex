\section{Enabling NUMA-Aware GPUs}

Designing large multi-socket GPUs that 
perform well across all workloads is not trivial despite the 
improvements that technologies like NVLink~\cite{NVLINK} and Unified Virtual 
Addressing~\cite{UVM} can provide. To improve the performance of our baseline 
multi-socket GPU implementation we 
propose two classes of improvements that leverage application phasing to reduced 
the effective NUMA penalty.  First, in 
Section~\ref{interconnect} we examine the ability of a switch connected GPU to 
dynamically change its interconnect signaling from a balanced up and down 
stream bandwidth design, to allowing flexible re-partitioning of these channels.  
When bi-directional bandwidth is observed to be under-utilized, the direction 
which has excess capacity can be re-assigned to support the oversubscribed 
channels. This allows any individual GPU to improve its bandwidth utilization at 
times when it finds itself most bandwidth constrained.

Second, because we observe that inter-GPU bandwidth has a strong correlation to 
overall multi-socket GPU performance we investigate the performance impact of 
enabling multi-socket GPU cache coherence in Section~\ref{caching}.  We propose 
extending the software based L1 coherence into the L2 caches of multi-socket 
GPUs.  By extending this coherence, the L2 caches of each GPU socket move from 
being memory-side, where coherence is not needed, to GPU-side, where coherence 
guarantees are required in the same manner as current GPU L1 caches.  We 
evaluate the effect of this coherence overhead on multi-socket GPUs and show 
that because of the GPU's weak coherence guarantees multi-socket coherence on 
GPUs should scale significantly better than traditional CPU coherence protocols.

Finally, we show that after extending coherence into the GPU L2 caches, the 
appropriate allocation of cache capacity between local memory bandwidth and 
remote memory bandwidth can not be decided at design time.  Due to the same 
application phasing that enabled dynamic interconnect rebalancing, we observe 
that individual GPU sockets should be free to balance their cache capacity 
between local and remote accesses to optimize performance.  When remote 
interconnect bandwidth is saturated, a GPU will skew its cache capacity toward 
the remote accesses,  however if that bandwidth is not saturated then optimizing 
cache hit rates to eliminate the largest number of memory requests is 
desirable.

Before diving into detailed proposals and results for our proposed
optimizations we first describe the multi-socket GPU runtime that consolidates
prior work from both the NUMA-GPU world and GPU then describe our multi-socket
GPU methodology.

% While the most basic multi-socket GPU 
% implementation may extend current GPU thread block scheduling and memory 
% management techniques across sockets, this results in sub-optimal 
% performance.  As shown in Figure~\ref{fig:motivation}, even applying NUMA 
% optimization techniques in the multi-socket GPU runtime such as first touch 
% page placement and Block CTA partitioning leaves significant performance on 
% the table compared to an hypothetical 4x larger (all resources scaled) GPU on many 
% benchmarks.

\subsection{A Locality Preserving GPU Runtime}
As demonstrated by Cabezas et al.~\cite{Cabezas2015} it is possible 
to design a framework and a runtime system that transparently decomposes GPU 
kernels in sub-kernels and executes them on multiple GPUs in parallel. On the 
Nvidia GPUs, this is doable by intercepting and remapping each single kernel 
call, GPU memory allocation, GPU memory copy and GPU wide synchronization issued 
by the CUDA driver. Special care needs to be put (i) to the GPU local memory 
fences which needs to be promoted to system level and (ii) to the 
sub-kernels CTA identifiers which needs to be properly corrected to 
reflect those in the original kernel call. 
 
In ~\cite{Cabezas2015} these two problems were solved by introducing  
code annotations and an additional source-to-source compiler which was also 
responsible for statically assigning data and computation on each GPU. In our 
work, we follow a very similar strategy but we do not use any additional 
compiler for two reasons (i) we use a simulated environment in which 
we can properly model the architectural requirements for system wide memory 
fences and sub-kernel to kernel CTA identifiers mapping (ii) we model a system 
which implements UVM page migration on demand, as recently introduced in the 
Tesla P100 GPU~\cite{P100}. UVM page migration allows to allocate 
all the memory upfront in system memory and to transparently migrate pages 
on the GPU memory on demand as soon as the first access (also called first 
touch) is performed.

Another approach for memory placement is to mimic what a single GPU does, 
which is to interleave memory at the granularity of single cache line to 
reduce memory camping effects and to maximize bandwidth. However interleaving 
at such fine granularity across multiple GPUs disrupts locality and it 
results in large amount of remote accesses even for regular and dense 
applications. A less fine grain solution is to interleave memory across GPUs 
at the granularity of pages. With respect to kernel partitioning, while on a 
single GPU fine grain dynamic assignment of CTAs to the SMs is performed to 
achieve good load balancing, extending it to a Multi-GPU system (for instance 
creating a large amount of small sub-kernels) results in huge overhead due to 
the cost of starting a single sub-kernel. For this reason, as done 
in~\cite{Cabezas2015}, we decompose a single kernel in only N sub-kernels, 
where N is the total number of GPUs in the system, and we assign to each GPU 
an equal amount of CTAs. We also select CTAs in each sub-kernel to be 
contiguous. This last choice could expose workload unbalance across 
sub-kernels, but it preserves locality present in many dense and regular 
applications where contiguous CTAs access contiguous memory. In the attempt 
to reduce unbalance an alternative solution consists in interleaving CTAs 
across the sub-kernels trying to follow the way pages interleave across 
memories. 

In Figure~\ref{fig:motivation} we show the relative performance a 4-Socket 
NUMA GPU with respect to a single GPU for two possible CTA scheduling and 
memory placement strategies above explained (see Section~\ref{methodology} 
for a detailed explanation of methodology and benchmark suites). With the 
\emph{green bars} we show relative performance of the natural extension to 
the traditional single GPU scheduling and memory interleaving to 
multi-GPU, which consists in memory page interleave and sub-kernel CTA 
interleave. With the \emph{light blue bars} instead we show the relative 
performance of the locality optimized GPU scheduling and memory placement 
consisting of contiguous block CTA scheduling and first touch page 
migration. We can clearly see that the Locality-Optimized solution almost always 
outperforms the traditional GPU scheduling and memory interleaving, in 
average performing 2x better. In general the problems of data placement and 
task placement (minimizing remote data access and ensuring good load balancing) 
can not be completely decoupled and pathological case can always be found. It 
is important to stress that we are not claiming the Locality-Optimized solution 
as a contribution of this paper (indeed very similar considerations and findings 
can be found in~\cite{Cabezas2015,Arunkumar2017}), instead we are setting up a 
baseline architecture in which we can apply and present our proposed 
optimizations in the next Sections.

Also in Figure~\ref{fig:motivation} we show with a \emph{red dash} the performance 
achievable by an hypothetical (unrealizable) 4x larger GPUs. This \emph{red dash} 
represent a good approximation of the maximum theoretical performance we 
could expect from a perfect NUMA system. We sorted the applications by the 
gap between relative performance of the Locality-Optimized and 
Hypothetical 4x larger GPUs. We can see that on the right side of the graph 
some applications (that have very good locality) already 
achieve or surpasses the maximum theoretical performance. In particular for the 
two far-most benchmarks on the right, the locality optimized solution outperforms 
the hypothetical 4x larger GPU due a better reuse of L2 data originated by 
the contiguous block scheduling of CTAs. For the applications on the left 
side of the Figure there is quite large gap between locality optimized and 
hypothetical. These are the applications in which either, locality does not 
exists or the Locality-Optimized solution can't capture it, 
resulting in large amount of remote transfers. In the rest of this paper we 
present techniques aimed to reduce this gap. To simplify the discussion we 
will exclude the benchmarks that already achieve 99\% of the theoretical 
performance when explaining these techniques. We will re-include them in the 
final plots to demonstrate that there was not performance regression due to 
our proposed optimizations.


\begin{table}[tp]
\begin{small}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value(s)} \\
\toprule
Num of GPU sockets & 4 \\
\midrule
Total number of SMs & 64 per GPU socket \\
\midrule
GPU Frequency & 1GHz \\
\midrule
Max number of Warps & 64 per SM \\
\midrule
Warp Scheduler & Greedy then Round Robin \\
\midrule
L1 Cache & Private, 128 KB per SM, 128B lines, 4-way, \\ 
& GPU-side SW-based coherency \\
\midrule
L2 Cache & Shared, 4MB per socket, 128B lines, 16-way, \\ 
& Memory-side non-coherent\\
\midrule
GPU--GPU Interconnect & 128GB/s per socket (64GB/s each direction) \\
& 8 lanes 8B wide each per direction \\
&128-cycle latency \\
\midrule
DRAM Bandwidth & 768GB/s per GPU socket\\
\midrule
DRAM Latency & 100ns \\
\toprule
\end{tabular}
\caption{Simulation parameters for evaluation of single and multi-socket GPU 
systems.}
\label{tab:setup}
\end{small}
\end{table}

\subsection{Simulation Methodology}
\label{methodology}

To evaluate the performance of multi-socket GPUs we use a proprietary, 
cycle-level, trace-driven simulator for GPU systems running single GPU CUDA 
applications. For our baseline, we model a single GPU that approximates the 
latest NVIDIA Pascal architecture~\cite{inside-pascal}. Each of the 
Streaming Multiprocessors (SM) is modeled as an in-order processor with 
multiple levels of cache hierarchy containing private, per-SM, L1 caches and 
multi-banked, shared, L2 cache. Each GPU is backed by its local high 
bandwidth DRAM memory. Our multi-socket GPU system contains four of these 
GPUs interconnected through a full bandwidth GPU switch as shown on 
Figure~\ref{fig:systemdiagram}. Table~\ref{tab:setup} stands as an overview 
of the simulation parameters.

We study our proposal using XXX benchmarks taken from a broad range of 
production codes based on the HPC CORAL benchmarks~\cite{coral}, graph 
applications from Lonestar~\cite{lonestar}, compute applications from 
Rodinia~\cite{Che2009}, in addition to several other in-house CUDA benchmarks. 
This set of workloads covers a wide spectrum of GPU applications used in machine 
learning, fluid dynamic, image manipulation, graph traversal, scientific 
computing, etc. We run our simulations until the completion of the entire 
applications or the number of kernels shown on Table~\ref{tab:numctas}.
Table~\ref{tab:numctas} also shows the average number of CTAs for each
application (weighed by the time spent on each kernel) and the memory 
footprint in MB.

\begin{table}[t]
\begin{small}
\centering
\begin{tabular}{lccc}
 \toprule
 \textbf{Benchmark} & \textbf{Kernels} & \textbf{Time-weighted} & \textbf{Memory} \\
& & \textbf{Average CTAs} & \textbf{(MB)} \\
 \toprule
ML-GoogLeNet-cudnn-L2 & 1 & 6272 & 1205 \\
ML-AlexNet-cudnn-L2 & 1 & 1250 & 832 \\
Optix-Raytracing & 1 & 3072 & 87 \\
Bitcoin-Crypto & 1 & 60 & 5898 \\
ML-OverFeat-cudann-L3 & 1 & 1800 & 388 \\
%Encription & 1 & 128 & 1 \\
ML-AlexNet-cudnn-L4 & 1 & 1014 & 32 \\
ML-AlexNet-ConvNet2 & 1 & 6075 & 97 \\
HPC-Namd2.9 & 1 & 3888 & 88 \\
HPC-RabbitCT & 1 & 131072 & 524 \\
Rodinia-Backprop & 2 & 4096 & 160 \\
Rodinia-Euler3D & 346 & 1008 & 25 \\
Rodinia-BFS & 24 & 1954 & 38 \\
Rodinia-Gaussian & 510 & 2599 & 78 \\
Rodinia-Hotspot & 1 & 7396 & 64 \\
Rodinia-Kmeans & 3 & 3249 & 221 \\
Rodnia-Pathfinder & 20 & 4630 & 1570 \\
Rodinia-Srad & 4 & 16384 & 98 \\
Lonestar-SP & 11 & 75 & 8 \\
HPC-Lulesh & 105 & 12202 & 578 \\
Lonestar-MST-graph & 87 & 770 & 86 \\
Lonestar-MST-mesh & 71 & 895 & 75 \\
HPC-CoMD & 350 & 3588 & 319 \\
HPC-CoMD-wa & 350 & 13691 & 393 \\
HPC-CoMD-ta & 350 & 5724 & 394 \\
HPC-HPGMG-UVM-opt & 359 & 10436 & 1975 \\
HPC-HPGMG & 317 & 10506 & 1571 \\
%HPC-HPGMG-UVM-base & 359 & 10728 & 1975 \\
HPC-Lulesh-Unstruct-small & 2000 & 435 & 19 \\
%HPC-Nekbone-medium & 510 & 3093 & 170 \\
Lonestar-SSSP-wln & 1000 & 60 & 21 \\
HPC-SNAP & 118 & 200 & 744 \\
HPC-Nekbone-large & 300 & 5583 & 294 \\
HPC-MiniAMR & 33 & 76033 & 2752 \\
HPC-MiniContact-large & 127 & 15423 & 257 \\
HPC-MiniContact-small & 500 & 250 & 21 \\
HPC-Lulesh-Unstruct-large & 200 & 4940 & 208 \\
HPC-AMG & 88 & 241549 & 3744 \\
HPC-RSBench & 1 & 7813 & 19 \\
HPC-MCB & 1 & 5001 & 162 \\
Lonestar-DMR & 3 & 82 & 248 \\
Lonestar-SSSP-wlc & 1300 & 163 & 21 \\
Lonestar-SSSP & 102 & 1046 & 38 \\
Stream-Triad & 5 & 699051 & 3146 \\
\toprule
\end{tabular}
\caption{Application footprint and average number of CTAs (threadblocks) 
available during time-weighted execution.}
\label{tab:numctas}
\end{small}
\end{table}