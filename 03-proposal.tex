\section{Enabling Basic TMS-GPU}

Designing TMS-GPUs that 
perform well across wide range of workloads is not trivial despite the 
improvements that technologies like NVLink~\cite{NVLINK} and Unified Virtual 
Addressing~\cite{UVM} bring to the table. 


% While the most basic multi-socket GPU 
% implementation may extend current GPU thread block scheduling and memory 
% management techniques across sockets, this results in sub-optimal 
% performance.  As shown in Figure~\ref{fig:motivation}, even applying NUMA 
% optimization techniques in the multi-socket GPU runtime such as first touch 
% page placement and Block CTA partitioning leaves significant performance on 
% the table compared to an hypothetical 4x larger (all resources scaled) GPU on many 
% benchmarks.

\subsection{Basic TMS-GPU Architecture}
As demonstrated by Cabezas et al.~\cite{Cabezas2015} it is possible 
to design a framework and a runtime system that transparently decomposes GPU 
kernels in sub-kernels and executes them on multiple GPUs in parallel. For
example, on Nvidia GPUs this can be implemented by intercepting and remapping each kernel 
call, GPU memory allocation, memory copy, and GPU-wide synchronization issued 
by the CUDA driver. Special care needs to be put (i) to the GPU local memory 
fences which needs to be promoted to system level and (ii) to the 
sub-kernels CTA identifiers which needs to be properly corrected to 
reflect those in the original kernel call. 
In ~\cite{Cabezas2015} these two problems were solved by introducing  
code annotations and an additional source-to-source compiler which was also 
responsible for statically assigning data and computation on each GPU. In our 
work, we follow a similar strategy but without using any additional 
compiler as we leverage a simulated environment with properly modeled 
the architectural requirements for system wide memory fences,
and sub-kernel to kernel CTA identifiers mapping. 

We assume and model a system with UVM page migration on demand~\cite{P100} that
allows transparent on-demand page migration from system memory to GPU memory as
soon as the first access (also called first-touch allocation) is performed.
Alternative approach for memory placement is memory interleaving at the
granularity of single cache line to reduce memory camping effects and to
maximize bandwidth. However interleaving at such fine granularity across
multiple GPUs reduces data locality, and results in growing number of remote
memory accesses. Another possible approach is a more cross-grain solution
interleaving memory across GPUs at the granularity of pages. 

While on a single GPU fine grain dynamic assignment of CTAs to SMs is performed
to achieve good load balancing, extending it to a Multi-GPU system is
sub-optimal due to the relatively high penalty of a single sub-kernel launch.
Therefore, similarly to~\cite{Cabezas2015}, we decompose a single kernel into N
sub-kernels, where N is the total number of GPUs in the system, and we assign
equal amount of contiguous CTAs to each GPU.  This design choice can potentially
expose workload unbalance across sub-kernels, but it preserves data locality
present in many applications where contiguous CTAs access contiguous memory. A
possible alternative solution can focus on interleaving CTAs across
sub-kernels, in attempt to mimic the way pages interleave across memories. 

\subsection{Basic TMS-GPU Performance}
In Figure~\ref{fig:motivation} we show the relative performance of a 4-Socket
TMS-GPU with respect to a single-GPU for two possible CTA scheduling and memory
placement strategies explained above. For a detailed explanation of methodology
and benchmark suites see Section~\ref{methodology}. With the \emph{green bars}
we show relative performance of the natural extension to the traditional single
GPU scheduling and memory interleaving to multi-GPU, which consists in memory
page interleave and sub-kernel CTA interleave. With the \emph{light blue bars}
instead we show the relative performance of the locality optimized GPU
scheduling and memory placement consisting of contiguous block CTA scheduling
and first touch page migration. We can clearly see that the Locality-Optimized
solution almost always outperforms the traditional GPU scheduling and memory
interleaving, in average performing 2$\times$ better. In general, the problems of data
placement and task placement (minimizing remote data access and ensuring good
load balancing) can not be completely decoupled and pathological case can
always be found. It is important to stress that we are not claiming the
Locality-Optimized solution as a contribution of this paper (very
similar considerations and findings can be found
in~\cite{Cabezas2015, Arunkumar2017}). Instead, we are setting up a basic TMS-GPU
architecture in which we can apply and present our proposed NUMA-aware optimizations.

Moreover in Figure~\ref{fig:motivation} we show with a \emph{red dash} the performance 
achievable by a hypothetical (unbuildable) 4$\times$ larger GPU. This \emph{red dash} 
represent a good approximation of the maximum theoretical performance we 
could expect from a perfect TMS-GPU. We sorted the applications by the 
gap between relative performance of the Locality-Optimized and 
Hypothetical 4x larger GPUs. We can see that on the right side of the graph 
some applications (that have very good locality) already 
achieve or surpass the maximum theoretical performance. In particular for the 
two far-most benchmarks on the right, the locality optimized solutions outperform 
the hypothetical 4x larger GPU due a higher cache data-locality originated by 
the contiguous block scheduling of CTAs. For the applications on the left 
side of the Figure there is quite large gap between locality optimized and 
hypothetical. These are the applications in which either, locality does not 
exist or the Locality-Optimized solution is not effective, 
resulting in large amount of remote data transfers. In the rest of this paper we 
present techniques aimed to reduce this gap. To simplify the discussion we 
will exclude the benchmarks that already achieve 99\% of the theoretical 
performance when explaining these techniques. We will re-include them in the 
final plots to demonstrate that there was not performance regression due to 
our proposed optimizations.


\begin{table}[tp]
\begin{small}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value(s)} \\
\toprule
Num of GPU sockets & 4 \\
\midrule
Total number of SMs & 64 per GPU socket \\
\midrule
GPU Frequency & 1GHz \\
\midrule
Max number of Warps & 64 per SM \\
\midrule
Warp Scheduler & Greedy then Round Robin \\
\midrule
L1 Cache & Private, 128 KB per SM, 128B lines, 4-way, \\ 
& GPU-side SW-based coherency \\
\midrule
L2 Cache & Shared, 4MB per socket, 128B lines, 16-way, \\ 
& Memory-side non-coherent\\
\midrule
GPU--GPU Interconnect & 128GB/s per socket (64GB/s each direction) \\
& 8 lanes 8B wide each per direction \\
&128-cycle latency \\
\midrule
DRAM Bandwidth & 768GB/s per GPU socket\\
\midrule
DRAM Latency & 100ns \\
\toprule
\end{tabular}
\caption{Simulation parameters for evaluation of single and multi-socket GPU 
systems.}
\label{tab:setup}
\end{small}
\end{table} 
 
\subsection{Simulation Methodology}
\label{sec:methodology}

To evaluate the performance of multi-socket GPUs we use a proprietary, 
cycle-level, trace-driven simulator for GPU systems running single GPU CUDA 
applications. For our baseline, we model a single GPU that approximates the 
latest NVIDIA Pascal architecture~\cite{inside-pascal}. Each of the 
Streaming Multiprocessors (SM) is modeled as an in-order processor with 
multiple levels of cache hierarchy containing private, per-SM, L1 caches and 
multi-banked, shared, L2 cache. Each GPU is backed by its local high 
bandwidth DRAM memory. Our multi-socket GPU system contains four of these 
GPUs interconnected through a full bandwidth GPU switch as shown on 
Figure~\ref{fig:systemdiagram}. Table~\ref{tab:setup} stands as an overview 
of the simulation parameters.

We study our proposal using 41 workloads taken from a broad range of 
production codes based on the HPC CORAL benchmarks~\cite{coral}, graph 
applications from Lonestar~\cite{lonestar}, compute applications from 
Rodinia~\cite{Che2009}, in addition to several other in-house CUDA benchmarks. 
This set of workloads covers a wide spectrum of GPU applications used in machine 
learning, fluid dynamic, image manipulation, graph traversal, scientific 
computing, etc. We run our simulations until the completion of the entire 
applications or the number of kernels shown on Table~\ref{tab:numctas}.
Table~\ref{tab:numctas} shown after the conclusions is provided to show
more complete data about each workload including average number of CTAs for each
application (weighed by the time spent on each kernel) and the memory 
footprint in MB.

